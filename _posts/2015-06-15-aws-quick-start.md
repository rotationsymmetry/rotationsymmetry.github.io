---
layout: post
title: A Quick Start Guide to Amazon Web Service
comments: true
categories: Spark
---

Over the years, Amazon Web Service (AWS) has evolved to include numerous components to meet your cloud computing needs. So AWS can be quite overwhelming if you just get started. This quick start guide will try to provide you with essential knowledge of AWS needed to set up a Spark cluster.

<!-- more -->

## AWS Account
You can sign up an AWS account at [http://aws.amazon.com](http://aws.amazon.com). Please note that AWS account is completely independent of your Amazon shopping account.

From the day you sign up, you can enjoy the free-tiered service for 12 months, which is sufficient for setting up a *test* Spark cluster.

After completing the sign-up process, you can go to the [AWS console] (https://console.aws.amazon.com/) to browse the available service.

![AWS Console](/images/aws/aws_console.png)


## Identify & Access Management (IAM)
If you log into AWS with the email and password of your AWS account, you have root privilege to AWS service available to you. For security purposes, you are strongly encouraged to set up users to perform dedicate tasks within AWS. This leads to first AWS components I will cover in this post: Identify & Access Management (IAM).

Simply put, IAM allows one to create new users and apply access policy to these users. A user can be authenticated by

* a login page with human entering the user name and password *or*
* a program with the `Access Key ID` and the `Secrete Access Key` of the user

Authenticating a user programmatically is important because AWS provides [Python API](http://aws.amazon.com/sdk-for-python/) for operations such as launching or terminating a virtual machine instance. So you can write script to perform task in AWS on behalf of a user after your script authenticate against AWS programmatically. Because of this advantage, when you create a new user in IAM, `Access Key ID` and `Secrete Access Key` will be generated by default and the password for login-page authenticate is optional.

Since `Access Key ID` and `Secrete Access Key` is for programmatically authentication, they are system-generated long and random strings, in order to provide protection against random guess. After all, you cannot expect a script to answer a [CAPTCHA challenge](https://en.wikipedia.org/wiki/CAPTCHA)  as a human does in a login page.

You can add an user using the [IAM](https://console.aws.amazon.com/iam/) module in the AWS console with the following steps:

* click `Users` tab on the left column
* click `Create New Users` button
*  Type in the user name and click `Create`
*  Save system generated `Access Key ID` and `Secrete Access Key` to some safe location. *Please save it; this is your only chance.*

![Access Key ID and Secrete Access Key](/images/aws/ID_and_secret_key.png)



## Elastic Compute Cloud (EC2)
Elastic Compute Cloud (EC2) is the AWS component that manages the virtual machine hardware and images as well as its life cycle (launch, terminate, etc).

EC2 virtual machines come with several series. The common ones are

* T series: machines that carry low load most of the time but with capability to burst to high performance
* M series: fixed performance machine for general use
* C series: fixed performance machine for number crunching
* R series: big memory

Within each series, you have choices of difference size (small, medium, large, etc) and generations. With the free-tiered service, you can access `t2.micro` machines, which is the least performant obviously.

AWS also offer a wide variety of images for your VM, including different distribution of Linux as well as Windows. For our purpose, sticking with the default Amazon Linux is the best bet. Amazon Linux includes JVM but does not include Hadoop or Spark. Fortunately, the Spark EC2 script will set up the Hadoop and Spark so you don't need to worry about this.

You can learn more about the life cycle of VM instance in the [EC2 documentation](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html). To summarize, you will need to launch to the VM before you can use. After you are done, you need to shut down the VM otherwise, Amazon will keep charging you. Considering we are trying to set up a cluster of machines including master and slaves, managing the life cycles appropriately can be a daunting task. Luckily again, the Spark EC2 script will to rescue and handle these for us. So I will not discuss how to launch or terminate VM in this guide.

Just one friendly note for the Spark EC2 script: by default the script will request to use `m1.large` machines. So if you don't the get charged for playing around, please specify the type of the machine in the Spark EC2 script.

Before I conclude this post, I would like to discuss one more set of keys. After the VM instance is launched, you and the Spark EC2 script will need to login into the instance. The AWS user authentication cannot be used because it only controls the AWS service but inside the VM instance. Instead, we will use the public key infrastructure (PKI) to authenticate `ssh` logins. A nice introduction of PKI in `ssh`can be found in the [Ubuntu documentation] (https://help.ubuntu.com/community/SSH/OpenSSH/Keys). For our purpose, we will

* click `Key Pairs` under the `Network & Security` section of EC2 module
* Type a name for the key pair, for example, 'sparkkeypair' and hit create
* save your private key 'sparkkeypair.pem' in a safe location

Finally, you will run `chmod 400 sparkkeypair.pem` to ensure that only you have read access to the file, making it truly private. Otherwise, `ssh` will not recognize the priate key.

All right, that is all you need to prepare for a Spark cluster in AWS. In the [next post](/2015/06/16/spark-ec2-script/), I will go over the Spark EC2 script to really step the cluster. Stay tuned.


![EC2](/images/aws/ec2.png)
